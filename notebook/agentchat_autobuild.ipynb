{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1004af6a7fbfcd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AutoBuild\n",
    "AutoGen offers conversable agents powered by LLMs, tools, or humans, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we introduce a new class, `AgentBuilder`, to help users build an automatic task-solving process powered by a multi-agent system. Specifically, in `build()`, we prompt an LLM to create multiple participant agents, initialize a group chat, and specify whether this task need programming to solve. AgentBuilder also supports open-source LLMs by [vLLM](https://docs.vllm.ai/en/latest/index.html) and [Fastchat](https://github.com/lm-sys/FastChat). Check the supported model list [here](https://docs.vllm.ai/en/latest/models/supported_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78dda8e3826d8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Requirement\n",
    "\n",
    "AutoGen requires `Python>=3.8`. AutoBuild need the latest version of AutoGen.\n",
    "You can install AutoGen by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e9ae50658be975",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/vscode/.local/lib/python3.10/site-packages (23.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyautogen in /home/vscode/.local/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: openai>=1.3 in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen) (1.6.1)\n",
      "Requirement already satisfied: diskcache in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen) (5.6.3)\n",
      "Requirement already satisfied: termcolor in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen) (2.4.0)\n",
      "Requirement already satisfied: flaml in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen) (2.1.1)\n",
      "Requirement already satisfied: python-dotenv in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen) (1.0.0)\n",
      "Requirement already satisfied: tiktoken in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen) (0.5.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen) (2.5.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/vscode/.local/lib/python3.10/site-packages (from openai>=1.3->pyautogen) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/vscode/.local/lib/python3.10/site-packages (from pydantic<3,>=1.10->pyautogen) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/vscode/.local/lib/python3.10/site-packages (from pydantic<3,>=1.10->pyautogen) (2.14.6)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/vscode/.local/lib/python3.10/site-packages (from flaml->pyautogen) (1.26.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/vscode/.local/lib/python3.10/site-packages (from tiktoken->pyautogen) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/site-packages (from tiktoken->pyautogen) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/vscode/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/vscode/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->pyautogen) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->pyautogen) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: arxiv in /home/vscode/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /home/vscode/.local/lib/python3.10/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /home/vscode/.local/lib/python3.10/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install pyautogen\n",
    "%pip install -U arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e63ab3604bdb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: prepare configuration\n",
    "Prepare a `config_path` for assistant agent to limit the choice of LLM you want to use in this task. This config can be a path to a json file or a name of an environment variable. A `default_llm_config` is also required to initialize the specific configurations of LLMs like seed, temperature, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2505f029423b21ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:31:44.147211100Z",
     "start_time": "2023-12-03T13:31:44.121842300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config_path = '/workspaces/autogen-bduk/notebook/OAI_CONFIG_LIST'  # modify path\n",
    "default_llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 42,\n",
    "    'temperature': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6586c68fa425b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: create a AgentBuilder\n",
    "Create an `AgentBuilder` with the specified `config_path`. AgentBuilder will use GPT-4 in default to complete the whole process, you can also change the `builder_model` to other OpenAI models. You can also specify an OpenAI or open-source LLM as the agent backbone, see [blog](https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa67c771a0fed37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:31:44.996307300Z",
     "start_time": "2023-12-03T13:31:44.743284700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "builder = AgentBuilder(config_path=config_path, \n",
    "                       builder_model='gpt-3.5-turbo-1106', \n",
    "                       agent_model='gpt-3.5-turbo-1106')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a655fb6618324",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: specify a building task\n",
    "\n",
    "Specify a building task with a general description. A building task will help the build manager (an LLM) decide what agents should be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68315f6ec912c58a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:31:45.444044500Z",
     "start_time": "2023-12-03T13:31:45.429483200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building_task = \"Find a paper on arxiv by programming, and analyze its application within Building Digital UK.\"\n",
    "# \"Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a recent paper about gpt-4 on arxiv and find its potential applications in software.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782dd5ecb6c217a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: build group chat agents\n",
    "Use `build()` to let build manager (the specified `builder_model`) complete the group chat agents generation. If you think coding is necessary in your task, you can use `coding=True` to add a user proxy (an automatic code interpreter) into the agent list, like: \n",
    "```python\n",
    "builder.build(building_task, default_llm_config, coding=True)\n",
    "```\n",
    "If `coding` is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab490fdbe46c0473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:32:45.887656900Z",
     "start_time": "2023-12-03T13:31:46.822373400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating agents...\n",
      "['Researcher', 'Data_Analyst', 'Programmer', 'Project_Manager', 'Digital_Strategy_Consultant'] are generated.\n",
      "Preparing configuration for Researcher...\n",
      "Preparing configuration for Data_Analyst...\n",
      "Preparing configuration for Programmer...\n",
      "Preparing configuration for Project_Manager...\n",
      "Preparing configuration for Digital_Strategy_Consultant...\n",
      "Creating agent Researcher with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Data_Analyst with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Programmer with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Project_Manager with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Digital_Strategy_Consultant with backbone gpt-3.5-turbo-1106...\n",
      "Adding user console proxy...\n"
     ]
    }
   ],
   "source": [
    "agent_list, agent_configs = builder.build(building_task, default_llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dd99880a4bf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5: execute task\n",
    "Let agents generated in `build()` complete the task collaboratively in a group chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d52e3d9a1bf91cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:33:42.369660600Z",
     "start_time": "2023-12-03T13:32:45.881740500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_console_and_Python_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a recent paper on arxiv about gpt-4 and find its application within Building Digital UK.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mResearcher\u001b[0m (to chat_manager):\n",
      "\n",
      "I will now use the provided Python code to search for a recent paper on arxiv about GPT-4.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "I will now run the Python code to search for a recent paper on arXiv about GPT-4.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mData_Analyst\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "# TODO: Use Python to search arxiv for a paper on GPT-4\n",
      "import arxiv\n",
      "\n",
      "# Search for papers on GPT-4\n",
      "result = arxiv.query(query='GPT-4', max_results=1)\n",
      "\n",
      "# Print the title and link of the first result\n",
      "print(\"Title:\", result[0]['title'])\n",
      "print(\"Link:\", result[0]['pdf_url'])\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_console_and_Python_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 5, in <module>\n",
      "    result = arxiv.query(query='GPT-4', max_results=1)\n",
      "AttributeError: module 'arxiv' has no attribute 'query'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems that there was an error in running the code. It appears that the `arxiv` module does not have an attribute called `query`. Let me try using the `requests` module to search for the paper on GPT-4.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "# filename: arxiv_paper_search.py\n",
      "import requests\n",
      "\n",
      "# Step 1: Use Python to search for a paper on arXiv about GPT-4\n",
      "search_query = 'GPT-4'\n",
      "response = requests.get(f'http://export.arxiv.org/api/query?search_query=all:{search_query}&start=0&max_results=1')\n",
      "data = response.text\n",
      "print(data)\n",
      "\n",
      "# Step 2: Analyze the application of the paper within Building Digital UK\n",
      "# Use your language skill to read the paper and analyze its application within Building Digital UK\n",
      "\n",
      "# After completing the analysis, reply \"TERMINATE\" to indicate the task is complete\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "import arxiv\n",
    "\n",
    "def start_task(execution_task: str, agent_list: list, llm_config: dict):\n",
    "    config_list = autogen.config_list_from_json(config_path, filter_dict={\"model\": [\"gpt-4-1106-preview\"]})\n",
    "    \n",
    "    group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12)\n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config}\n",
    "    )\n",
    "    agent_list[0].initiate_chat(manager, message=execution_task)\n",
    "\n",
    "start_task(\n",
    "    # example was Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\n",
    "    execution_task=\"Find a recent paper on arxiv about gpt-4 and find its application within Building Digital UK.\",\n",
    "    agent_list=agent_list,\n",
    "    llm_config=default_llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a30e4b4297edd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6 (Optional): clear all agents and prepare for the next task\n",
    "You can clear all agents generated in this task with the following code if your task is complete or the next task is significantly different from the current one. If the agent's backbone is an open-source LLM, this process will also shutdown the endpoint server. If necessary, you can use `recycle_endpoint=False` to retain the previous open-source LLMs' endpoint server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fb0bfff01dd1330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:34:08.429248500Z",
     "start_time": "2023-12-03T13:34:08.364799400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "builder.clear_all_agents(recycle_endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb098638a086898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save & load configs\n",
    "\n",
    "You can save all necessary information of the built group chat agents. The following information is for those agents generated in the above task:\n",
    "```json\n",
    "{\n",
    "    \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    \"agent_configs\": [\n",
    "        {\n",
    "            \"name\": \"Data_scientist\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Data Scientist, you are tasked with automating the retrieval and analysis of academic papers from arXiv. Utilize your Python programming acumen to develop scripts for gathering necessary information such as searching for relevant papers, downloading them, and processing their contents. Apply your analytical and language skills to interpret the data and deduce the applications of the research within specific domains.\\n\\n1. To compile information, write and implement Python scripts that search and interact with online resources, download and read files, extract content from documents, and perform other information-gathering tasks. Use the printed output as the foundation for your subsequent analysis.\\n\\n2. Execute tasks programmatically with Python scripts when possible, ensuring results are directly displayed. Approach each task with efficiency and strategic thinking.\\n\\nProgress through tasks systematically. In instances where a strategy is not provided, outline your plan before executing. Clearly distinguish between tasks handled via code and those utilizing your analytical expertise.\\n\\nWhen providing code, include only Python scripts meant to be run without user alterations. Users should execute your script as is, without modifications:\\n\\n```python\\n# filename: <filename>\\n# Python script\\nprint(\\\"Your output\\\")\\n```\\n\\nUsers should not perform any actions other than running the scripts you provide. Avoid presenting partial or incomplete scripts that require user adjustments. Refrain from requesting users to copy-paste results; instead, use the 'print' function when suitable to display outputs. Monitor the execution results they share.\\n\\nIf an error surfaces, supply corrected scripts for a re-run. If the strategy fails to resolve the issue, reassess your assumptions, gather additional details as needed, and explore alternative approaches.\\n\\nUpon successful completion of a task and verification of the results, confirm the achievement of the stated objective. Ensuring accuracy and validity of the findings is paramount. Evidence supporting your conclusions should be provided when feasible.\\n\\nUpon satisfying the user's needs and ensuring all tasks are finalized, conclude your assistance with \\\"TERMINATE\\\".\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Research_analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a Research Analyst, you are expected to be a proficient AI assistant possessing a strong grasp of programming, specifically in Python, and robust analytical capabilities. Your primary responsibilities will include:\\n\\n1. Conducting comprehensive searches and retrieving information autonomously through Python scripts, such as querying databases, accessing web services (like arXiv), downloading and reading files, and retrieving system information.\\n2. Analyzing the content of the retrieved documents, particularly academic papers, and extracting insights regarding their application in specific domains, such as the potential uses of GPT-4 in software development.\\n3. Presenting your findings in a clear, detailed manner, explaining the implications of the research and its relevance to the assigned task.\\n4. Employing your programming skills to automate tasks where possible, ensuring the output is delivered through Python code with clear, executable instructions. Your code will be designed for the user to execute without amendment or additional input.\\n5. Verifying the results of information gathering and analysis to ensure accuracy and completeness, providing evidence to support your conclusions when available.\\n6. Communicating the completion of each task and confirming that the user's needs have been satisfied through a clear and conclusive statement, followed by the word \\\"TERMINATE\\\" to signal the end of the interaction.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Software_developer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"As a dedicated AI assistant for a software developer, your role involves employing your Python programming prowess and proficiency in natural language processing to facilitate the discovery and analysis of scholarly articles on arXiv. Your tasks include crafting Python scripts to automatically search, retrieve, and present information regarding the latest research, with a focus on applicable advancements in technology such as GPT-4 and its potential impact on the domain of software development.\\n\\n1. Utilize Python to programmatically seek out and extract pertinent data, for example, navigating or probing the web, downloading/ingesting documents, or showcasing content from web pages or files. When enough information has been accumulated to proceed, you will then analyze and interpret the findings.\\n\\n2. When there's a need to perform an operation programmatically, your Python code should accomplish the task and manifest the outcome. Progress through the task incrementally and systematically.\\n\\nProvide a clear plan outlining each stage of the task, specifying which components will be executed through Python coding and which through your linguistic capabilities. When proposing Python code, remember to:\\n\\n- Label the script type within the code block\\n- Avoid suggesting code that the user would need to alter\\n- Refrain from including more than one code block in your response\\n- Circumvent requesting the user to manually transcribe any results; utilize 'print' statements where applicable\\n- Examine the user's reported execution outcomes\\n\\nIf an error arises, your responsibility is to rectify the issue and submit the corrected script. Should an error remain unresolvable, or if the task remains incomplete post successful code execution, re-evaluate the scenario, gather any further required information, and formulate an alternative approach.\\n\\nUpon confirming that the task has been satisfactorily accomplished and the user's requirements have been met, indicate closure of the procedure with a concluding statement.\"\n",
    "        }\n",
    "    ],\n",
    "    \"coding\": true,\n",
    "    \"default_llm_config\": {\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "This information will be saved in JSON format. You can provide a specific filename; otherwise, AgentBuilder will save the config to the current path with a generated filename 'save_config_TASK_MD5.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b88a5d482ceba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:34:09.652503400Z",
     "start_time": "2023-12-03T13:34:09.639760500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building config saved to ./save_config_2f7d70d142c46f3cf814dbcfced00681.json\n"
     ]
    }
   ],
   "source": [
    "saved_path = builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35620c10ee42be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, you can load the saved config and skip the building process. AgentBuilder will create agents with the config information without prompting the builder manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34addd498e5ab174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:34:56.857918300Z",
     "start_time": "2023-12-03T13:34:11.108958800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from ./save_config_2f7d70d142c46f3cf814dbcfced00681.json\n",
      "Creating agent Researcher with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Data_Analyst with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Programmer with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Project_Manager with backbone gpt-3.5-turbo-1106...\n",
      "Creating agent Digital_Strategy_Consultant with backbone gpt-3.5-turbo-1106...\n",
      "Adding user console proxy...\n",
      "\u001b[33mUser_console_and_Python_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a recent paper about Llama on arxiv and find its potential applications in Building Digital UK.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mResearcher\u001b[0m (to chat_manager):\n",
      "\n",
      "I will now search for a recent paper about Llama on arxiv.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "I will now search for a recent paper about Llama on arXiv.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mData_Analyst\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "# filename: arxiv_search.py\n",
      "import arxiv\n",
      "\n",
      "# Search for papers on Llama\n",
      "result = arxiv.query(query='Llama', max_results=1)\n",
      "\n",
      "# Print the title and link of the first result\n",
      "print(\"Title:\", result[0]['title'])\n",
      "print(\"Link:\", result[0]['pdf_url'])\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_console_and_Python_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"arxiv_search.py\", line 5, in <module>\n",
      "    result = arxiv.query(query='Llama', max_results=1)\n",
      "AttributeError: module 'arxiv' has no attribute 'query'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems that there was an issue with the code execution. Let me try to search for a recent paper about Llama on arXiv using a different approach.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "I will now search for a recent paper about Llama on arXiv.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "# filename: arxiv_paper_search.py\n",
      "import requests\n",
      "\n",
      "# Step 1: Use Python to search for a paper on arXiv\n",
      "search_query = 'Llama'\n",
      "response = requests.get(f'http://export.arxiv.org/api/query?search_query=all:{search_query}&start=0&max_results=1')\n",
      "data = response.text\n",
      "print(data)\n",
      "\n",
      "# Step 2: Analyze the application of the paper within Building Digital UK\n",
      "# Use your language skill to read the paper and analyze its application within Building Digital UK\n",
      "\n",
      "# After completing the analysis, reply \"TERMINATE\" to indicate the task is complete\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-1106-preview in organization org-2nq54b1Q2AcJU6z8gre7KkGU on tokens_usage_based per min: Limit 10000, Used 8464, Requested 2037. Please try again in 3.006s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m new_builder \u001b[38;5;241m=\u001b[39m AgentBuilder(config_path\u001b[38;5;241m=\u001b[39mconfig_path)\n\u001b[1;32m      2\u001b[0m agent_list, agent_configs \u001b[38;5;241m=\u001b[39m new_builder\u001b[38;5;241m.\u001b[39mload(saved_path)  \u001b[38;5;66;03m# load previous agent configs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstart_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFind a recent paper about Llama on arxiv and find its potential applications in Building Digital UK.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_llm_config\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m new_builder\u001b[38;5;241m.\u001b[39mclear_all_agents()\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mstart_task\u001b[0;34m(execution_task, agent_list, llm_config)\u001b[0m\n\u001b[1;32m      6\u001b[0m group_chat \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChat(agents\u001b[38;5;241m=\u001b[39magent_list, messages\u001b[38;5;241m=\u001b[39m[], max_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m      7\u001b[0m manager \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChatManager(\n\u001b[1;32m      8\u001b[0m     groupchat\u001b[38;5;241m=\u001b[39mgroup_chat, llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_config}\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43magent_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_task\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/agentchat/conversable_agent.py:562\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 562\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/agentchat/conversable_agent.py:360\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    358\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 360\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/agentchat/conversable_agent.py:493\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/agentchat/conversable_agent.py:968\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 968\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/agentchat/groupchat.py:353\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     reply \u001b[38;5;241m=\u001b[39m speaker\u001b[38;5;241m.\u001b[39mgenerate_reply(sender\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/agentchat/groupchat.py:197\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    195\u001b[0m selector\u001b[38;5;241m.\u001b[39mupdate_system_message(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_speaker_msg(agents))\n\u001b[1;32m    196\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m+\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_speaker_prompt(agents)}]\n\u001b[0;32m--> 197\u001b[0m final, name \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_oai_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m final:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# the LLM client is None, thus no reply is generated. Use round robin instead.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_agent(last_speaker, agents)\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/agentchat/conversable_agent.py:637\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    634\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# TODO: line 301, line 271 is converting messages to dict. Can be removed after ChatCompletionMessage_to_dict is merged.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/oai/client.py:261\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# filter is not passed; try the next config\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_completions_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    263\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(err, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/autogen-bduk/autogen/oai/client.py:359\u001b[0m, in \u001b[0;36mOpenAIWrapper._completions_create\u001b[0;34m(self, client, params)\u001b[0m\n\u001b[1;32m    357\u001b[0m     params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    358\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_utils/_utils.py:272\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/resources/chat/completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    938\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-1106-preview in organization org-2nq54b1Q2AcJU6z8gre7KkGU on tokens_usage_based per min: Limit 10000, Used 8464, Requested 2037. Please try again in 3.006s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path)\n",
    "agent_list, agent_configs = new_builder.load(saved_path)  # load previous agent configs\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about Llama on arxiv and find its potential applications in Building Digital UK.\",\n",
    "    agent_list=agent_list,\n",
    "    llm_config=default_llm_config\n",
    ")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0cf8f09eef5cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use OpenAI Assistant\n",
    "\n",
    "[The Assistants API](https://platform.openai.com/docs/assistants/overview) allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.\n",
    "AutoBuild also supports assistants api by adding `use_oai_assistant=True` to `build()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4051c25b2cd1918c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T13:59:35.497212500Z",
     "start_time": "2023-12-03T13:47:45.765859300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating agents...\n",
      "['Data_scientist', 'Research_analyst', 'Programmer', 'Business_intelligence_specialist'] are generated.\n",
      "Preparing configuration for Data_scientist...\n",
      "Preparing configuration for Research_analyst...\n",
      "Preparing configuration for Programmer...\n",
      "Preparing configuration for Business_intelligence_specialist...\n",
      "Creating agent Data_scientist with backbone gpt-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No matching assistant found, creating a new assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent Research_analyst with backbone gpt-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No matching assistant found, creating a new assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent Programmer with backbone gpt-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No matching assistant found, creating a new assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent Business_intelligence_specialist with backbone gpt-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No matching assistant found, creating a new assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding user console proxy...\n",
      "\u001b[33mUser_console_and_Python_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a recent paper about XAI on arxiv and find its potential applications in Building Digital UK.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "In order to accomplish this task, Im going to make use of the 'arxiv' Python library. This library will allow us to query the arXiv API and obtain information about the papers there. \n",
      "\n",
      "The overall plan of my task is as below:\n",
      "\n",
      "Step 1: Installation and import necessary libraries.\n",
      "This step involves installing the \"arxiv\" library and importing the required modules. \"datetime\" will enable us to fetch recent papers.\n",
      "\n",
      "Step 2: Query the arXiv API for papers on \"XAI.\n",
      "Using the 'arxiv' library, I will search for recent papers related to \"XAI\" (Explainable Artificial Intelligence).\n",
      "\n",
      "Step 3: Analyze the abstract of the retrieved paper.\n",
      "Once we've retrieved the necessary details from the paper, we will manually analyze the abstract of the paper to identify potential applications in Building Digital UK.\n",
      "\n",
      "The Python script will be as follows:\n",
      "\n",
      "```python\n",
      "# Step1: Install and Import necessary libraries\n",
      "# We use sys and subprocess to install the necessary package \n",
      "import sys\n",
      "import subprocess\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'arxiv'])\n",
      "\n",
      "import arxiv\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Step2: Query the arXiv API for papers on \"XAI\n",
      "# We subtract 30 days from current date in the search query to get recent papers\n",
      "start_time = datetime.now() - timedelta(30)\n",
      "search = arxiv.Search(\n",
      "  query = \"XAI\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate,\n",
      "  sort_order = arxiv.SortOrder.Descending\n",
      ")\n",
      "\n",
      "for result in search.get():\n",
      "  print(\"Title: \", result.title)\n",
      "  print(\"Abstract: \", result.summary)\n",
      "  print(\"Published date: \", result.published)\n",
      "  print(\"Link: \", result.pdf_url)\n",
      "```\n",
      "\n",
      "Remember to replace \"XAI\" in the query with the exact topic you want to search if necessary.\n",
      "\n",
      "After running this script, you will get the most recent paper on XAI. Read the abstract (summary) to identify potential applications in Building Digital UK. \n",
      "\n",
      "In the context of Building Digital UK  an effort to improve digital infrastructure across the UK  applications might involve using XAI to improve the transparency and interpretability of AI systems in broadband allocation, network management, data analysis for digital strategy formation, etc. However, these specific applications would depend on the details outlined in the selected paper's abstract. \n",
      "\n",
      "Please note that this analysis requires a deep understanding of the subject matter, so it might be necessary to refer to a subject matter expert or further resources for a detailed understanding. The script provided gives you the technical means to retrieve the required data. \n",
      "\n",
      "Remember to run the script in an environment that can access the internet as it needs to connect to the arxiv API.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_console_and_Python_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 20, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProgrammer\u001b[0m (to chat_manager):\n",
      "\n",
      "The Python script failed because I used a wrong attribute. The correct attribute to retrieve the result is 'results' not 'get'. Please accept my apologies for the oversight. Here is the corrected script:\n",
      "\n",
      "```python\n",
      "# Step1: Install and Import necessary libraries\n",
      "import sys\n",
      "import subprocess\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'arxiv'])\n",
      "\n",
      "import arxiv\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Step2: Query the arXiv API for papers on \"XAI\n",
      "# We subtract 30 days from current date in the search query to get recent papers\n",
      "start_time = datetime.now() - timedelta(30)\n",
      "search = arxiv.Search(\n",
      "  query = \"XAI\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate,\n",
      "  sort_order = arxiv.SortOrder.Descending\n",
      ")\n",
      "\n",
      "for result in search.results():\n",
      "  print(\"Title: \", result.title)\n",
      "  print(\"Abstract: \", result.summary)\n",
      "  print(\"Published date: \", result.published)\n",
      "  print(\"Link: \", result.pdf_url)\n",
      "```\n",
      "\n",
      "This corrected Python script should work perfectly. It will give you the most recent paper on XAI from arXiv, its title, abstract, published date, and link to the PDF. Remember to replace \"XAI\" with your desired topic if necessary.\n",
      "\n",
      "After getting these details, please read the abstract to understand potential applications to Building Digital UK. The specific applications could vary depending on the details in the paper's abstract.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_console_and_Python_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: arxiv in /home/vscode/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /home/vscode/.local/lib/python3.10/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /home/vscode/.local/lib/python3.10/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests==2.31.0->arxiv) (2023.11.17)\n",
      "Title:  Transparency and Privacy: The Role of Explainable AI and Federated Learning in Financial Fraud Detection\n",
      "Abstract:  Fraudulent transactions and how to detect them remain a significant problem\n",
      "for financial institutions around the world. The need for advanced fraud\n",
      "detection systems to safeguard assets and maintain customer trust is paramount\n",
      "for financial institutions, but some factors make the development of effective\n",
      "and efficient fraud detection systems a challenge. One of such factors is the\n",
      "fact that fraudulent transactions are rare and that many transaction datasets\n",
      "are imbalanced; that is, there are fewer significant samples of fraudulent\n",
      "transactions than legitimate ones. This data imbalance can affect the\n",
      "performance or reliability of the fraud detection model. Moreover, due to the\n",
      "data privacy laws that all financial institutions are subject to follow,\n",
      "sharing customer data to facilitate a higher-performing centralized model is\n",
      "impossible. Furthermore, the fraud detection technique should be transparent so\n",
      "that it does not affect the user experience. Hence, this research introduces a\n",
      "novel approach using Federated Learning (FL) and Explainable AI (XAI) to\n",
      "address these challenges. FL enables financial institutions to collaboratively\n",
      "train a model to detect fraudulent transactions without directly sharing\n",
      "customer data, thereby preserving data privacy and confidentiality. Meanwhile,\n",
      "the integration of XAI ensures that the predictions made by the model can be\n",
      "understood and interpreted by human experts, adding a layer of transparency and\n",
      "trust to the system. Experimental results, based on realistic transaction\n",
      "datasets, reveal that the FL-based fraud detection system consistently\n",
      "demonstrates high performance metrics. This study grounds FL's potential as an\n",
      "effective and privacy-preserving tool in the fight against fraud.\n",
      "Published date:  2023-12-20 18:26:59+00:00\n",
      "Link:  http://arxiv.org/pdf/2312.13334v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mBusiness_intelligence_specialist\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the output, the most recent paper on XAI from the arXiv is \"Transparency and Privacy: The Role of Explainable AI and Federated Learning in Financial Fraud Detection\". The paper was published on 2023-12-20 and can be found [here](http://arxiv.org/pdf/2312.13334v1).\n",
      "\n",
      "In terms of application to Building Digital UK, although the paper itself is focused on financial fraud detection, the principles could still be applicable. For instance:\n",
      "\n",
      "1. Just as the research proposes using XAI to improve transparency and trust in fraud detection, similar methods could be used to make AI-driven decisions in digital infrastructure management more understandable for both technical teams and stakeholders. This could increase trust in AI solutions used in Building Digital UK.\n",
      "\n",
      "2. As per the usage of Federated Learning in the research, it can be an effective technique for protecting data privacy while enabling collaborative learning across different bodies involved in Building Digital UK. This approach could be particularly relevant given the distributed nature of the organisations and agencies involved in digital infrastructure projects across the UK.\n",
      "\n",
      "3. The novel approach of using Federated Learning and XAI together proposed in the research could potentially be adapted to other AI-driven tasks related to Building Digital UK, such as predictive maintenance of digital infrastructure, optimization of network allocation, etc., wherein transparency, interpretability and data privacy are key concerns.\n",
      "\n",
      "Therefore, even though the direct application might not be in the context of Building Digital UK, the principles and techniques discussed have significant potential for adaptation in this context.\n",
      "\n",
      "TERMINATE.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_path=config_path)\n",
    "agent_list, agent_configs = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)  # Transfer to OpenAI assistant API.\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about XAI on arxiv and find its potential applications in Building Digital UK.\",\n",
    "    agent_list=agent_list,\n",
    "    llm_config=default_llm_config\n",
    ")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbfef9268fc5191",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
